{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8a915d-f445-4402-9ad7-3907b5597ef6",
   "metadata": {},
   "source": [
    "## BERTopic model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeeef57-168d-470b-86fd-d718a9c3d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "em= 'TurkuNLP/sbert-cased-finnish-paraphrase'\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "import glob\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f19b0a-623c-4825-9fd4-b5a87bfd2437",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d22640-8354-4cf4-bf09-f5444dadee1d",
   "metadata": {},
   "source": [
    "### Read in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438fe1a4-c8b4-4d78-b6e5-df49fc4b744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in train data\n",
    "trainpath= \"../data/s24_forest_train.csv\"\n",
    "df = pd.read_csv(trainpath, sep=\"\\t\")\n",
    "documents=list(df[\"text\"].drop_duplicates())\n",
    "print(\"Training on\", len(documents), \"documents. \\n\")\n",
    "\n",
    "# and test data\n",
    "testpath= \"../data/s24_forest_test.csv\"\n",
    "df2 = pd.read_csv(testpath, sep=\"\\t\")\n",
    "documents2=list(df2[\"text\"].drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a130c-0dbb-4d28-b604-25ff0d3ff286",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a function to clean url addresses and lowercase words that were totally in uppercase\n",
    "# punctiation was corrected so that sentences were correctly separated by a space after a dot\n",
    "\n",
    "def clean_doc(doc):\n",
    "    # Pattern to match any letters separated by a dot\n",
    "    pattern = r\"([a-zöåäA-ZÅÄÖ]+)\\.([a-zåäöA-ZÅÄÖ]+)\"\n",
    "    pattern2 = r\"([a-zåäöA-ZÅÄÖ]+)\\,([a-zöäåA-ZÅÄÖ]+)\"\n",
    "\n",
    "    words = doc.split()\n",
    "    #print(words)\n",
    "    doc_list= [word for word in words if \"www\" not in word]\n",
    "    #words = doc.split()\n",
    "    doc_list= [word for word in doc_list if \"http\" not in word]\n",
    "    doc = \" \".join(doc_list)\n",
    "    # Replace with a space after the dot\n",
    "    if \"http\" and \"www\" not in doc:\n",
    "        doc = re.sub(pattern, r\"\\1. \\2\", doc)\n",
    "        doc = re.sub(pattern2, r\"\\1, \\2\", doc)\n",
    "    doc_clean = re.sub(r\"\\s+\", \" \", doc).strip()\n",
    "    doc_clean = re.sub('[€]', 'euro', doc_clean)\n",
    "    doc_clean = re.sub('[*\"]', '', doc_clean)\n",
    "    words = doc_clean.split()\n",
    "    # Convert words with all uppercase letters to lowercase\n",
    "    processed_doc = [word.lower() if word.isupper() else word for word in words]\n",
    "    return \" \".join(processed_doc)\n",
    "    \n",
    "documents2 = [clean_doc(d) for d in documents2]\n",
    "documents = [clean_doc(d) for d in documents]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1ecce2-b7e7-4050-941f-468630e3dff9",
   "metadata": {},
   "source": [
    "## nr of topics for different models\n",
    "\n",
    "\n",
    "- turku-nlp umap kmeans\n",
    "\n",
    "- turku-nlp pca kmeans\n",
    "\n",
    "- xlm-r umap kmeans\n",
    "\n",
    "- xlm-r pca kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454763d4-c4d0-4c87-8b0b-735e9f6204ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_out =\"data/slurm-24538287.out\"\n",
    "\n",
    "with open(k_out, \"r\") as fp:\n",
    "    k_res=fp.readlines()\n",
    "    \n",
    "k_res = [k for k in k_res if k !=\"\\n\"]\n",
    "models = [\"turku-umap\",\"turku-pca\", \"xlm-umap\",\"xlm-pca\"]\n",
    "n=0\n",
    "scores=[]\n",
    "for k in k_res:\n",
    "\n",
    "    if k.startswith(\"Loading\"):\n",
    "        n+=1\n",
    "        print(k)\n",
    "        wcss=[]\n",
    "        silhouette=[]\n",
    "    elif \"Done!\" in k:\n",
    "        scores.append([wcss,silhouette])\n",
    "    elif k[0].isdigit():\n",
    "        #print(k.split())\n",
    "        wcss.append(float(k.split()[0]))\n",
    "        silhouette.append(float(k.split()[1]))\n",
    "        #print(k)\n",
    "#scores       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e418e-95e0-414f-8184-0c38621cb629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot elbow and silhouette scores\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a 2x2 grid of subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))  # 2 rows, 2 columns\n",
    "\n",
    "k_values = [50,75,100,125,150,175,200,225,250,275,300]\n",
    "# Plot data on each subplot\n",
    "axes[0, 0].plot(k_values,scores[0][0], 'bx-', label=\"elbow\")\n",
    "axes[0, 0].set_title(models[0])\n",
    "# Create a secondary y-axis\n",
    "ax2 = axes[0, 0].twinx()\n",
    "ax2.scatter(k_values,scores[0][1], color=\"red\", s=10, marker=\"x\")\n",
    "ax2.set_ylabel(\"silhouette\", color=\"red\")\n",
    "ax2.tick_params(axis='y', labelcolor=\"red\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "axes[0, 1].plot(k_values,scores[1][0], 'bx-', label=\"elbow\")\n",
    "axes[0, 1].set_title(models[1])\n",
    "# Create a secondary y-axis\n",
    "ax2 = axes[0, 1].twinx()\n",
    "ax2.scatter(k_values,scores[1][1], color=\"red\", s=10, marker=\"x\")\n",
    "ax2.set_ylabel(\"silhouette\", color=\"red\")\n",
    "ax2.tick_params(axis='y', labelcolor=\"red\")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "axes[1, 0].plot(k_values,scores[2][0], 'bx-', label=\"elbow\")\n",
    "axes[1, 0].set_title(models[2])\n",
    "# Create a secondary y-axis\n",
    "ax2 = axes[1, 0].twinx()\n",
    "ax2.scatter(k_values,scores[2][1], color=\"red\", s=10, marker=\"x\")\n",
    "ax2.set_ylabel(\"silhouette\", color=\"red\")\n",
    "ax2.tick_params(axis='y', labelcolor=\"red\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "axes[1, 1].plot(k_values,scores[3][0], 'bx-', label=\"elbow\")\n",
    "axes[1, 1].set_title(models[3])\n",
    "# Create a secondary y-axis\n",
    "ax2 = axes[1, 1].twinx()\n",
    "ax2.scatter(k_values,scores[3][1], color=\"red\", s=10, marker=\"x\")\n",
    "ax2.set_ylabel(\"silhouette\", color=\"red\")\n",
    "ax2.tick_params(axis='y', labelcolor=\"red\")\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a935ffc-17b3-406f-a982-48fbd8f8fb3d",
   "metadata": {},
   "source": [
    "Used nr of topics:\n",
    "- turku-umap and xlm-umap = 175\n",
    "- turku-pca 200\n",
    "- xlm-pca 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a2ea3-7217-484f-ad92-c38d880d2804",
   "metadata": {},
   "source": [
    "# Explore a trained model\n",
    "\n",
    "Code in this section is based on tutorials from https://maartengr.github.io/BERTopic/getting_started/quickstart/quickstart.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a7368-1d07-4c8c-ba2b-fa549ef10439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## load in a model\n",
    "path='/scratch/project_2008526/telmap/suomi24/bertopicmodel/model_sbert-cased-finnish-paraphrase_umap_kmeans'\n",
    "topic_model = BERTopic.load(path+\"/picklemodel\")\n",
    "\n",
    "# get keywords per topic\n",
    "topic_dict= topic_model.get_topics()\n",
    "t=[]\n",
    "kw=[]\n",
    "for k,v in topic_dict.items():\n",
    "    t.append(k)\n",
    "    wordlist= [i[0] for i in v]\n",
    "    kw.append(\" \".join(wordlist))\n",
    "    \n",
    "kwdf = pd.DataFrame({\"Topic\":t, \"Keywords\":kw})\n",
    "kwdf.head() # show topic keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c547f8-de64-4645-a79e-356cc9ce75bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see basic document info\n",
    "\n",
    "document_info = topic_model.get_document_info(documents)\n",
    "df=topic_model.get_topic_info()\n",
    "topics = list(df[\"Topic\"])\n",
    "\n",
    "# print mean topic size and the size of the smallest topic\n",
    "print(df[df[\"Topic\"]!=-1][[\"Count\"]].mean().values, df[df[\"Topic\"]!=-1][[\"Count\"]].min().values)\n",
    "\n",
    "# -1 topic size (only applicable for hdbscan models)\n",
    "if -1 in df[\"Topic\"]:\n",
    "    print(\"Trash\")\n",
    "    print(len(df[df[\"Topic\"]==-1]))\n",
    "df.sort_values(by=\"Count\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ac9c8-34ba-4cfa-8d0b-266c8be0643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out small topics\n",
    "df_filtered= df[df[\"Count\"]>29]\n",
    "\n",
    "topics_filtered=list(df_filtered[\"Topic\"])\n",
    "\n",
    "docsdf = document_info[document_info[\"Topic\"].isin(topics_filtered)]\n",
    "docs=list(docsdf[\"Document\"])\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e13d27c-ebd2-4498-a3eb-c014d510a1d6",
   "metadata": {},
   "source": [
    "The hierarchical topic tree was used to get an impression of the semantic coherence of the created topics. \n",
    "(i.e. Does the branch structure make sense for a human evaluator?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b495bee3-6bdf-4f1d-9be2-635ccafe3814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hierarchical topics\n",
    "hierarchical_topics = topic_model.hierarchical_topics(documents)\n",
    "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a50b73-7627-4b40-aa17-ad89a779a257",
   "metadata": {},
   "source": [
    "# go through topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da7b54-d2ff-4adc-9efa-a864fc449310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "document_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6269c2e-8c5b-4386-b3d4-9ba3f495cf13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print representative documents\n",
    "representative = document_info[(document_info[\"Representative_document\"]==True)&(document_info[\"Topic\"]==173)]\n",
    "print(list(representative.sample(1)[\"Document\"]))\n",
    "representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3562f68a-3196-4c73-a6c9-5d49753f84c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate probabilities for a kmeans model\n",
    "\n",
    "topic_distr, _ = topic_model.approximate_distribution(documents, min_similarity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aebc1d-0778-41e0-b378-fd51543353a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the max probability for each (i.e. the probability of the assigned topic)\n",
    "topic_max = [np.max(t) for t in topic_distr]\n",
    "topic_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa431e84-ef0f-46f6-be40-431909f382c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of max topic probabilities\n",
    "print(np.median(topic_max), np.mean(topic_max), np.max(topic_max), np.min(topic_max),np.std(topic_max))\n",
    "\n",
    "#counts, bins = np.histogram(topic_max)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(topic_max, bins=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123ab99-ae0e-45de-9fd0-b7f4e2ee34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column for the probabilities\n",
    "document_info[\"Topic_probability\"] = topic_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b64e0c8-9bf0-4f9a-b2cc-59f26c2b0714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out documents with a low probability ==> get more representative documents\n",
    "filtered_docs= document_info[document_info[\"Topic_probability\"] >= 0.02]\n",
    "\n",
    "# how many documents do we have left?\n",
    "print(\"After filtering:\", len(filtered_docs) , \"docs.\")\n",
    "filtered_docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5960e16a-a4f2-4c01-9598-ac298e2a18da",
   "metadata": {},
   "source": [
    "### Exploring topics\n",
    "Based on topic keywords, relevant topics for further analysis were identified. Topics that seemed to share a common theme were investigated together. E.g. topics 24, 73, 88, and 151 seemed to be about clearcuttings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc41c3-3d20-4c37-ae5e-0dd88c806333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read documents from a topic\n",
    "# 24, 73, 88, 151 (clearcuttings)\n",
    "a_topic=random.choice(topics)\n",
    "a_topic=151\n",
    "\n",
    "# 24 : avohakkuut tuhoavat monimuotoisuutta VS viherhippien täytyy antaa ammattilaisten tehdä työtään\n",
    "# 73 => puukauppa\n",
    "# 88 avohakkuu mielipiteitä, lyhyitä\n",
    "# 151 avohakkuu ja suojellut alueet\n",
    "print(\"Topic:\",a_topic)\n",
    "print(\"Keywords:\" ,filtered_docs[filtered_docs[\"Topic\"]==a_topic][\"Name\"].head(1))\n",
    "temp = filtered_docs[filtered_docs[\"Topic\"]==a_topic]\n",
    "print(\"nr of docs:\", len(temp),\"\\n\")\n",
    "\n",
    "# print a sample\n",
    "example_texts = temp.sample(7)[\"Document\"]\n",
    "for e in example_texts:\n",
    "    print(e,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5daf311-3a22-4d35-9a3f-fe36afd92b9f",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbdb115-0498-4128-942d-d6d508123991",
   "metadata": {},
   "outputs": [],
   "source": [
    "nature_conservation =[2,10,43,72,173]\n",
    "hakkuutopics=[24,73,88,151]\n",
    "subset_docs=document_info[document_info[\"Topic\"].isin(hakkuutopics)]\n",
    "\n",
    "# print out topic probability information\n",
    "print(subset_docs[\"Topic_probability\"].mean(),subset_docs[\"Topic_probability\"].std() )\n",
    "print(subset_docs[\"Topic_probability\"].min(),subset_docs[\"Topic_probability\"].max() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383f7a5a-c3b9-4893-8cbd-bb281027c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a threshold to filter out irrelevant documents\n",
    "threshold=subset_docs[\"Topic_probability\"].mean()-1*subset_docs[\"Topic_probability\"].std()\n",
    "filtered_subset_docs=subset_docs[subset_docs[\"Topic_probability\"]>threshold]\n",
    "print(len(filtered_subset_docs), len(subset_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe34f0-c201-4f11-bfee-f2d0824de4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make reduced embeddings for plotting\n",
    "\n",
    "embedding_model ='TurkuNLP/sbert-cased-finnish-paraphrase'\n",
    "\n",
    "epath=\"/scratch/project_2008526/telmap/suomi24/turkunlp_train_embeddings.h5\"\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "print(f\"Loading embeddings '{epath}'. \\n\")\n",
    "with h5py.File(epath, \"r\") as f:\n",
    "    embeddings = f[\"embeddings\"][:]\n",
    "    \n",
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b011f6-0649-4945-ac1c-d4516b82ffbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize the wanted topics\n",
    "topic_model.visualize_documents(documents, topics=hakkuutopics, custom_labels=labels,reduced_embeddings=reduced_embeddings, width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bab72c-dbf3-4f4b-ac93-dd72116e2a19",
   "metadata": {},
   "source": [
    "#### Modified function to remove axes (no needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6418afb-4bc0-4396-9bac-aeaf66750a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from umap import UMAP\n",
    "from typing import List, Union\n",
    "\n",
    "\n",
    "def visualize_documents(\n",
    "    topic_model,\n",
    "    docs: List[str],\n",
    "    topics: List[int] = None,\n",
    "    embeddings: np.ndarray = None,\n",
    "    reduced_embeddings: np.ndarray = None,\n",
    "    sample: float = None,\n",
    "    hide_annotations: bool = False,\n",
    "    hide_document_hover: bool = False,\n",
    "    custom_labels: Union[bool, str] = False,\n",
    "    title: str = \"<b>Documents and Topics</b>\",\n",
    "    width: int = 1200,\n",
    "    height: int = 750,\n",
    "):\n",
    "    \"\"\"Visualize documents and their topics in 2D.\n",
    "\n",
    "    Arguments:\n",
    "        topic_model: A fitted BERTopic instance.\n",
    "        docs: The documents you used when calling either `fit` or `fit_transform`\n",
    "        topics: A selection of topics to visualize.\n",
    "                Not to be confused with the topics that you get from `.fit_transform`.\n",
    "                For example, if you want to visualize only topics 1 through 5:\n",
    "                `topics = [1, 2, 3, 4, 5]`.\n",
    "        embeddings: The embeddings of all documents in `docs`.\n",
    "        reduced_embeddings: The 2D reduced embeddings of all documents in `docs`.\n",
    "        sample: The percentage of documents in each topic that you would like to keep.\n",
    "                Value can be between 0 and 1. Setting this value to, for example,\n",
    "                0.1 (10% of documents in each topic) makes it easier to visualize\n",
    "                millions of documents as a subset is chosen.\n",
    "        hide_annotations: Hide the names of the traces on top of each cluster.\n",
    "        hide_document_hover: Hide the content of the documents when hovering over\n",
    "                             specific points. Helps to speed up generation of visualization.\n",
    "        custom_labels: If bool, whether to use custom topic labels that were defined using\n",
    "                       `topic_model.set_topic_labels`.\n",
    "                       If `str`, it uses labels from other aspects, e.g., \"Aspect1\".\n",
    "        title: Title of the plot.\n",
    "        width: The width of the figure.\n",
    "        height: The height of the figure.\n",
    "\n",
    "    \"\"\"\n",
    "    topic_per_doc = topic_model.topics_\n",
    "\n",
    "    # Sample the data to optimize for visualization and dimensionality reduction\n",
    "    if sample is None or sample > 1:\n",
    "        sample = 1\n",
    "\n",
    "    indices = []\n",
    "    for topic in set(topic_per_doc):\n",
    "        s = np.where(np.array(topic_per_doc) == topic)[0]\n",
    "        size = len(s) if len(s) < 100 else int(len(s) * sample)\n",
    "        indices.extend(np.random.choice(s, size=size, replace=False))\n",
    "    indices = np.array(indices)\n",
    "\n",
    "    df = pd.DataFrame({\"topic\": np.array(topic_per_doc)[indices]})\n",
    "    df[\"doc\"] = [docs[index] for index in indices]\n",
    "    df[\"topic\"] = [topic_per_doc[index] for index in indices]\n",
    "\n",
    "    # Extract embeddings if not already done\n",
    "    if sample is None:\n",
    "        if embeddings is None and reduced_embeddings is None:\n",
    "            embeddings_to_reduce = topic_model._extract_embeddings(df.doc.to_list(), method=\"document\")\n",
    "        else:\n",
    "            embeddings_to_reduce = embeddings\n",
    "    else:\n",
    "        if embeddings is not None:\n",
    "            embeddings_to_reduce = embeddings[indices]\n",
    "        elif embeddings is None and reduced_embeddings is None:\n",
    "            embeddings_to_reduce = topic_model._extract_embeddings(df.doc.to_list(), method=\"document\")\n",
    "\n",
    "    # Reduce input embeddings\n",
    "    if reduced_embeddings is None:\n",
    "        umap_model = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric=\"cosine\").fit(embeddings_to_reduce)\n",
    "        embeddings_2d = umap_model.embedding_\n",
    "    elif sample is not None and reduced_embeddings is not None:\n",
    "        embeddings_2d = reduced_embeddings[indices]\n",
    "    elif sample is None and reduced_embeddings is not None:\n",
    "        embeddings_2d = reduced_embeddings\n",
    "\n",
    "    unique_topics = set(topic_per_doc)\n",
    "    if topics is None:\n",
    "        topics = unique_topics\n",
    "\n",
    "    # Combine data\n",
    "    df[\"x\"] = embeddings_2d[:, 0]\n",
    "    df[\"y\"] = embeddings_2d[:, 1]\n",
    "\n",
    "    # Prepare text and names\n",
    "    if isinstance(custom_labels, str):\n",
    "        names = [[[str(topic), None]] + topic_model.topic_aspects_[custom_labels][topic] for topic in unique_topics]\n",
    "        names = [\"_\".join([label[0] for label in labels[:4]]) for labels in names]\n",
    "        names = [label if len(label) < 30 else label[:27] + \"...\" for label in names]\n",
    "    elif topic_model.custom_labels_ is not None and custom_labels:\n",
    "        names = [topic_model.custom_labels_[topic + topic_model._outliers] for topic in unique_topics]\n",
    "    else:\n",
    "        names = [\n",
    "            f\"{topic}\"\n",
    "            for topic in unique_topics\n",
    "        ]\n",
    "\n",
    "    # Visualize\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Outliers and non-selected topics\n",
    "    non_selected_topics = set(unique_topics).difference(topics)\n",
    "    if len(non_selected_topics) == 0:\n",
    "        non_selected_topics = [-1]\n",
    "\n",
    "    selection = df.loc[df.topic.isin(non_selected_topics), :]\n",
    "    selection[\"text\"] = \"\"\n",
    "    selection.loc[len(selection), :] = [\n",
    "        None,\n",
    "        None,\n",
    "        selection.x.mean(),\n",
    "        selection.y.mean(),\n",
    "        \"Other documents\",\n",
    "    ]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scattergl(\n",
    "            x=selection.x,\n",
    "            y=selection.y,\n",
    "            hovertext=selection.doc if not hide_document_hover else None,\n",
    "            hoverinfo=\"text\",\n",
    "            mode=\"markers+text\",\n",
    "            name=\"other\",\n",
    "            showlegend=False,\n",
    "            marker=dict(color=\"#CFD8DC\", size=5, opacity=0.5),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Selected topics\n",
    "    for name, topic in zip(names, unique_topics):\n",
    "        if topic in topics and topic != -1:\n",
    "            selection = df.loc[df.topic == topic, :]\n",
    "            selection[\"text\"] = \"\"\n",
    "\n",
    "            if not hide_annotations:\n",
    "                selection.loc[len(selection), :] = [\n",
    "                    None,\n",
    "                    None,\n",
    "                    selection.x.mean(),\n",
    "                    selection.y.mean(),\n",
    "                    name,\n",
    "                ]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scattergl(\n",
    "                    x=selection.x,\n",
    "                    y=selection.y,\n",
    "                    hovertext=selection.doc if not hide_document_hover else None,\n",
    "                    hoverinfo=\"text\",\n",
    "                    text=selection.text,\n",
    "                    mode=\"markers+text\",\n",
    "                    name=name,\n",
    "                    textfont=dict(\n",
    "                        size=12,\n",
    "                    ),\n",
    "                    marker=dict(size=5, opacity=0.5),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Add grid in a 'plus' shape\n",
    "    x_range = (\n",
    "        df.x.min() - abs((df.x.min()) * 0.15),\n",
    "        df.x.max() + abs((df.x.max()) * 0.15),\n",
    "    )\n",
    "    y_range = (\n",
    "        df.y.min() - abs((df.y.min()) * 0.15),\n",
    "        df.y.max() + abs((df.y.max()) * 0.15),\n",
    "    )\n",
    "    \n",
    "    fig.add_annotation(x=x_range[0], y=sum(y_range) / 2, text=\"D1\", showarrow=False, yshift=10)\n",
    "    fig.add_annotation(y=y_range[1], x=sum(x_range) / 2, text=\"D2\", showarrow=False, xshift=10)\n",
    "\n",
    "    # Stylize layout\n",
    "    fig.update_layout(\n",
    "        template=\"simple_white\",\n",
    "        title={\n",
    "            \"text\": \"\",\n",
    "            \"x\": 0.5,\n",
    "            \"xanchor\": \"center\",\n",
    "            \"yanchor\": \"top\",\n",
    "            #\"font\": dict(size=22, color=\"Black\"),\n",
    "        },\n",
    "        width=width,\n",
    "        height=height,\n",
    "        \n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(visible=False, showline=False, showgrid=False, zeroline=False)\n",
    "    fig.update_yaxes(visible=False, showline=False, showgrid=False, zeroline=False)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d75623-df84-4bdc-b822-6d4c3333be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "myfig=visualize_documents(topic_model, documents, topics=hakkuutopics, custom_labels=labels,reduced_embeddings=reduced_embeddings, width=600,height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3630d4d-2265-47ed-9f77-d6b1c79325d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "myfig.update_layout(\n",
    "    xaxis=dict(\n",
    "        title=\"\",  # Set x-axis title\n",
    "        #range=[-5, 5],                # Set x-axis range\n",
    "        showgrid=True,               # Show gridlines\n",
    "        zeroline=False,\n",
    "        showline=False # Hide zero line\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        \n",
    "        title=\"\",  # Set y-axis title\n",
    "        #range=[3.5, 13.5],                 # Set y-axis range\n",
    "        showline=False,                # Show axis line\n",
    "        mirror=False,# Mirror axis line on top/bottom or left/right\n",
    "        zeroline=False,\n",
    "        showgrid=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc5e612-fab7-4f1c-82ef-a3a0b883a1c9",
   "metadata": {},
   "source": [
    "## Thematic groups\n",
    "\n",
    "BERTopic only assigns one topic per document, so to work around that, I looked at topic probabilities and selected documents for which the probability of a certain document was high. This way, you could retrieve also documents beyond the topic label assignments. (See https://maartengr.github.io/BERTopic/getting_started/distribution/distribution.html#example )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ffdefc-7312-4389-acf1-979edc48eca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mhoito=[27 ,99 ,112 ,136 ,30]\n",
    "ilmasto_nielut=[34,64,79,127]\n",
    "mkauppa=[42,106]\n",
    "hakkuut=[24,151,88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d90877d-f1c1-483f-ac0c-a1ff062758dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.median(topic_distr))\n",
    "print(np.mean(topic_distr))\n",
    "print(np.std(topic_distr))\n",
    "print(np.mean(topic_distr) - np.std(topic_distr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca755bf-cba9-4956-9b64-a5cb0da8621a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hakkuut\n",
    "\n",
    "hakkuu_true = [x[24] > 0.02 or x[151] > 0.02 or  x[88] > 0.02 for x in topic_distr]\n",
    "ilmasto_true = [x[64] > 0.02 or x[127] > 0.02 for x in topic_distr]\n",
    "nielut_true = [x[34] > 0.02 or x[79] > 0.02 for x in topic_distr]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1e3c7-e58b-4d57-93e5-a3d9e9088ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_info[\"hakkuu\"] = hakkuu_true\n",
    "document_info[\"ilmasto\"] = ilmasto_true\n",
    "document_info[\"nielut\"] = nielut_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf9d3c8-bf6a-4c2f-a290-ad47f8c771ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# see some climate-related posts\n",
    "print(len(document_info[document_info[\"ilmasto\"]==True][\"Document\"].values))\n",
    "print(document_info[document_info[\"ilmasto\"]==True][\"Document\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ed396-e748-4366-a93a-0b7e7db4be30",
   "metadata": {},
   "source": [
    "## CLimate viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b6957d-5c9d-48e0-afcc-c0edb784db21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hiilinielut_df=document_info[document_info[\"nielut\"]==True]\n",
    "#ilmasto_df=document_info[document_info[\"ilmasto\"]==True]\n",
    "#hakkuu_df=document_info[document_info[\"hakkuu\"]==True]\n",
    "document_info[\"timestamps\"] =train[\"date\"]\n",
    "document_info['year'] = pd.DatetimeIndex(document_info['timestamps']).year\n",
    "document_info['month'] = pd.DatetimeIndex(document_info['timestamps']).month\n",
    "\n",
    "hakkuut=[24, 88, 151]\n",
    "clim=document_info[document_info[\"Topic\"].isin(hakkuut)][[\"Document\",\"Topic\",\"Top_n_words\", \"Topic_probability\",\"timestamps\",\"year\",\"month\"]]\n",
    "print(clim[\"Topic_probability\"].mean())\n",
    "print(clim[\"Topic_probability\"].std())\n",
    "\n",
    "# filter documents with a threshold of mean - 2*SD\n",
    "print(clim[\"Topic_probability\"].mean()-2*clim[\"Topic_probability\"].std())\n",
    "mean_std=clim[\"Topic_probability\"].mean()-2*clim[\"Topic_probability\"].std()\n",
    "clim=clim[clim[\"Topic_probability\"]>mean_std]\n",
    "print(len(clim))\n",
    "print(clim[\"Document\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9793959-5c37-4662-9968-191da5057b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read through the lowest-probability documents to see if the threshold makes sense\n",
    "clim.sort_values(by=\"Topic_probability\")[\"Document\"].head().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e742f4-1cec-4809-af90-73c2e03dfc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the development of climate post counts over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4ac438-8637-4e5e-9dc7-3245fa7f3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate a date range from 2014-01-01 to 2020-12-31 with monthly frequency\n",
    "date_range = pd.date_range(start=\"2014-01-01\", end=\"2020-12-31\", freq=\"M\")\n",
    "\n",
    "# Create a DataFrame with year, month, and year_month columns\n",
    "timedf = pd.DataFrame({\n",
    "    \"year\": date_range.year,\n",
    "    \"month\": date_range.month,\n",
    "    \"year_month\": date_range.strftime(\"%Y_%m\")\n",
    "})\n",
    "\n",
    "timedf['month'] = timedf['month'].astype(str).str.zfill(2)\n",
    "print(len(timedf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ebd73d-c29a-450a-9a2e-98ec57aaff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aggregating and calculating the standard deviation\n",
    "per_month = clim.groupby(by=[\"year\", \"month\"]).size().reset_index(name=\"count\")\n",
    "per_month['month'] = per_month['month'].astype(str).str.zfill(2)\n",
    "per_month[\"year_month\"] = per_month[\"year\"].astype(str) + \"_\" + per_month[\"month\"].astype(str)\n",
    "per_month= per_month.merge(timedf, on=[\"year\",\"month\",\"year_month\"], how=\"outer\")#.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07e7deb-7f85-422e-a4e5-858e802dee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate standard deviation (example: simulate with sample data)\n",
    "# If you have raw data instead of counts, replace `.size()` with an appropriate aggregation function\n",
    "\n",
    "rolling_window = 8  # You can adjust the window size\n",
    "per_month[\"rolling_std\"] = per_month[\"count\"].fillna(0).rolling(window=rolling_window, min_periods=1).std()\n",
    "std_dev = per_month[\"count\"].fillna(0).std()#.reset_index()  # Replace 'value_column' with your column name\n",
    "#per_month = per_month.merge(std_dev, on=[\"year\", \"month\"])\n",
    "\n",
    "# Define x, y, and error\n",
    "x = per_month[\"year_month\"]\n",
    "y = per_month[\"count\"].fillna(0)\n",
    "error = per_month[\"rolling_std\"]\n",
    "\n",
    "# Plotting the data\n",
    "#ax=plt.figure(figsize=(9, 5))\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "plt.plot(x, y, '.-', label='Count')  # Black line for the main plot\n",
    "plt.fill_between(x, y - error, y + error, color='lightblue', alpha=0.6, label='Standard Deviation')  # Fill for std deviation\n",
    "\n",
    "# Beautify the plot\n",
    "plt.xticks(rotation=90, fontsize=8)  # Rotate x-axis labels for clarity\n",
    "custom_labels = [f\"{label[-2:]}-{label[:4]}\" for i, label in enumerate(x)]  # Example custom label format\n",
    "\n",
    "# You can specify positions for the ticks (e.g., every 6th tick)\n",
    "plt.xticks(ticks=np.arange(0, len(x), step=2), labels=custom_labels[::2], rotation=90, fontsize=8)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Time, months\")\n",
    "plt.ylabel(\"Nr of posts\")\n",
    "#plt.title(\"Monthly Count with Standard Deviation\")\n",
    "#plt.legend()\n",
    "plt.ylim((0,55))\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()\n",
    "\n",
    "fig.savefig('figures/climate.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c44ee-0475-4341-96cb-b054ace6b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_month=clim.groupby(by=[\"year\",\"month\"]).size().reset_index(name=\"count\")\n",
    "per_month['month'] = per_month['month'].astype(str).str.zfill(2)\n",
    "per_month[\"year_month\"]= per_month[\"year\"].astype(str)+\"_\"+per_month[\"month\"].astype(str)\n",
    "per_month.plot.scatter(x=\"year_month\", y=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de04a72-296f-4d24-b380-0fbda78c8bf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hiilinielut_df.groupby([\"year\",\"month\"]).size().reset_index().tail(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41784e1a-8cdd-4918-846a-1efe9d21da02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(hiilinielut_df[(hiilinielut_df[\"year\"]==2018) &(hiilinielut_df[\"month\"]==10)][\"Document\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d12ad-0dd0-49c3-96ad-7555c5594275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(hakkuu_df[\"Document\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff361e-c82d-4c92-b965-d2a9aabb5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiilinielut_df.groupby([\"year\",\"month\"]).size().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cbf2ed-bbff-41e6-b99d-89f17199dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ilmasto_df.groupby([\"year\",\"month\"]).size().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cb295b-7549-45ac-bdd2-bb2b734069e3",
   "metadata": {},
   "source": [
    "## Topics over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f236bc33-2cae-4372-b29c-564f85b13061",
   "metadata": {},
   "outputs": [],
   "source": [
    "mhoito=[27 ,99 ,112 ,136 ,30]\n",
    "ilmasto_nielut=[34,64,79,127]\n",
    "mkauppa=[42,106]\n",
    "hakkuut=[24,151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a301049-8f0e-48b6-abe8-def836a60b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps=list(train[\"date\"])\n",
    "topics_over_time = topic_model.topics_over_time(documents, timestamps, nr_bins=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bb3eaa-19d2-48c3-be72-22b21d33b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_over_time(topics_over_time, topics=hakkuut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27740a56-847c-488b-b191-8c53070f332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_over_time(topics_over_time, topics=ilmasto_nielut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a932eb2b-de99-45ca-a25c-f4a4a6f74c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "topic_model.visualize_topics_over_time(topics_over_time, topics=[10,72])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7dfda4-1088-423e-ae25-c31d045aa289",
   "metadata": {},
   "source": [
    "## topic reduction\n",
    "\n",
    "Not done for the current paper, but BERTopic offers a function that merges topics together. As we had many overlapping topics, it might make sense to do this.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9d36dc-5836-4d59-9b61-d82dc48ce628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further reduce topics\n",
    "topic_model.reduce_topics(documents, nr_topics=130)\n",
    "\n",
    "# Access updated topics\n",
    "new_topics = topic_model.topics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d70a63-b000-4ea8-bb85-eccf58f369d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical topics\n",
    "hierarchical_topics = topic_model.hierarchical_topics(documents)\n",
    "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f738b1c-c999-4a69-b232-57aa848da057",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = topic_model.get_document_info(documents)\n",
    "docs_per_topics = T.groupby([\"Topic\"]).apply(lambda x: x.index,include_groups=False).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e34b85-d156-483d-b885-1b1aad2875f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_per_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5b9b0-952c-430e-88d6-6bf773419c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# representative docs\n",
    "\n",
    "topic_model.get_representative_docs(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e079df-3386-4c4e-8c26-4294678c14da",
   "metadata": {},
   "source": [
    "## Coherence score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26848922-44b8-4413-ae62-9b0eaa92a584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coherence score\n",
    "# c_v from \n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "# Tokenize your documents\n",
    "tokenized_docs = [doc.split() for doc in documents]  # or use a tokenizer for more complex preprocessing\n",
    "\n",
    "# Create a dictionary from the tokenized documents\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "\n",
    "\n",
    "# compute coherence score\n",
    "tops = topic_model.get_topics()\n",
    "top_words_per_topic = [[word for word, _ in topic_model.get_topic(topic)] for topic in tops.keys()]\n",
    "\n",
    "coherence_model = CoherenceModel(\n",
    "    topics=top_words_per_topic,\n",
    "    texts=[doc.split() for doc in documents],  # Tokenized documents\n",
    "    dictionary=dictionary, \n",
    "    coherence='c_v'   # 'c_v' is popular for topic models, but you can experiment with others\n",
    ")\n",
    "\n",
    "# Calculate the coherence score\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(\"Coherence Score:\", coherence_score)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16e9462-cef7-4769-9671-ad568209f1d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics = document_info[\"Topic\"].sort_values().unique()\n",
    "quality = {\"relevant\":{\"topics\":[],\"quality\":[]}, \"irrelevant\":{\"topics\":[],\"quality\":[]}}\n",
    "for t in topics[:5]:\n",
    "    temp = document_info[document_info[\"Topic\"]==t]\n",
    "    example_texts = temp.sample(20)[\"Document\"]\n",
    "    for e in example_texts:\n",
    "        print(e)\n",
    "    print(\"is the topic relevant? (1/0)\")\n",
    "    relevance = input()\n",
    "    print(\"is the quality good (1), ok (2), or bad (3)?\")\n",
    "    q= input()\n",
    "    if relevance ==\"1\":\n",
    "        print(\"Relevant\",\"\\n\")\n",
    "        quality[\"relevant\"][\"topics\"].append(t)\n",
    "        \n",
    "        quality[\"relevant\"][\"quality\"].append(q)\n",
    "\n",
    "    else: \n",
    "        quality[\"irrelevant\"][\"topics\"].append(t)\n",
    "        \n",
    "        quality[\"irrelevant\"][\"quality\"].append(q)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
